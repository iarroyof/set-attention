model:
  implementation: set_only
  attention_family: dense
  backend: exact
  vocab_size: 0
  d_model: 256
  num_layers: 4
  num_heads: 8
  window_size: 64
  stride: 64
  dropout: 0.1
  max_seq_len: 256
  router_topk: 4
  feature_mode: hashed_counts
  router_type: learned
  pooling:
    mode: mean
  geometry:
    enabled: true
    apply_as_bias: false
    apply_in_phi_attn: true
  sig_gating:
    enabled: false
training:
  epochs: 5
  lr: 0.0003
  seed: 0
data:
  dataset: wikitext2
  # limit: 50
  batch_size: 16
  seq_len: 256
