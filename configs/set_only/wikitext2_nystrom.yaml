model:
  implementation: set_only
  attention_family: linear
  backend: nystrom
  vocab_size: 0
  d_model: 256
  num_layers: 4
  num_heads: 8
  window_size: 32
  stride: 16
  dropout: 0.1
  attn_dropout: null
  resid_dropout: null
  ffn_dropout: null
  max_seq_len: 256
  router_type: uniform
  router_topk: 0
  backend_params:
    num_landmarks: 8
  feature_mode: geometry_only
  gamma: 1.0
  beta: 0.0

data:
  dataset: wikitext2
  batch_size: 16
  seq_len: 256

training:
  epochs: 5
  lr: 0.0003
