model:
  implementation: set_only
  attention_family: dense
  backend: exact
  architecture: transformer_lm
  vocab_size: 0

  d_model: 384
  num_heads: 8
  num_layers: 6
  dim_feedforward: 1536
  dropout: 0.1
  attn_dropout: null
  resid_dropout: null
  ffn_dropout: null
  max_seq_len: 512

  window_size: 16
  stride: 8

  pooling:
    mode: soft_trimmed_boltzmann
    tau: 0.1
    q: 0.85

  feature_mode: hashed_counts
  router_type: learned
  router_topk: 16

data:
  dataset: wikitext2
  batch_size: 16
  seq_len: 512

training:
  epochs: 10
  lr: 0.0003
  warmup_steps: 1000
