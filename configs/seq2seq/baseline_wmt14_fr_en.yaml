task: seq2seq

model:
  family: baseline_token
  architecture: transformer_seq2seq
  vocab_size: 0
  d_model: 256
  nhead: 8
  num_layers: 4
  dim_feedforward: 1024
  dropout: 0.1
  max_seq_len: 128
  seq2seq:
    shared_vocab: true

data:
  seq_dataset: wmt14_fr_en
  batch_size: 16
  max_len: 128
  limit: 200

training:
  epochs: 2
  lr: 0.0003
