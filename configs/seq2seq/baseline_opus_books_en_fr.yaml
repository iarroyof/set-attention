task: seq2seq

model:
  implementation: baseline_token
  attention_family: dense
  backend: exact
  architecture: transformer_seq2seq
  vocab_size: 0
  d_model: 256
  num_heads: 8
  num_layers: 4
  dim_feedforward: 1024
  dropout: 0.1
  attn_dropout: null
  resid_dropout: null
  ffn_dropout: null
  max_seq_len: 256
  seq2seq:
    shared_vocab: true

data:
  seq_dataset: opus_books_en_fr
  batch_size: 16
  seq_len: 256
  #limit: 200

training:
  epochs: 5
  lr: 0.0003
  seed: 0
