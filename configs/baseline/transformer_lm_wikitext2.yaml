model:
  family: baseline_token
  architecture: transformer_lm
  vocab_size: 50257
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1
  max_seq_len: 512

data:
  dataset: wikitext2
  batch_size: 16
  seq_len: 512

training:
  epochs: 10
  lr: 3e-4
  warmup_steps: 1000
