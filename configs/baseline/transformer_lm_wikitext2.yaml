model:
  implementation: baseline_token
  attention_family: dense
  backend: exact
  architecture: transformer_lm
  vocab_size: 0
  d_model: 384
  num_heads: 8
  num_layers: 6
  dim_feedforward: 1536
  dropout: 0.1
  attn_dropout: null
  resid_dropout: null
  ffn_dropout: null
  max_seq_len: 512

data:
  dataset: wikitext2
  batch_size: 16
  seq_len: 512

training:
  epochs: 10
  lr: 0.0003
  warmup_steps: 1000
