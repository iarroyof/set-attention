                        _seen
                    except NameError:
                        _seen = 0
                    idxs = range(_seen, _seen + src.size(0))
                    src_vals_list = []
                    src_offs_list = [0]
                    tgt_vals_list = []
                    tgt_offs_list = [0]
                    src_sigs_list = []
                    tgt_sigs_list = []
                    base_s = 0; base_t = 0
                    for bi in idxs:
                        # Offsets are per-sequence, so slice one window per example
                        sa, sb = src_offs_all[bi].item(), src_offs_all[bi+1].item()
                        ta, tb = tgt_offs_all[bi].item(), tgt_offs_all[bi+1].item()
                        sv = src_vals_all[sa:sb]; tv = tgt_vals_all[ta:tb]
                        src_vals_list.append(sv); tgt_vals_list.append(tv)
                        base_s += sv.numel(); base_t += tv.numel()
                        src_offs_list.append(base_s); tgt_offs_list.append(base_t)
                        src_sigs_list.append(src_sigs_all[bi:bi+1]); tgt_sigs_list.append(tgt_sigs_all[bi:bi+1])
                    src_vals_tok = (torch.cat(src_vals_list, dim=0) if src_vals_list else torch.empty(0, dtype=torch.long, device=device))
                    tgt_vals_tok = (torch.cat(tgt_vals_list, dim=0) if tgt_vals_list else torch.empty(0, dtype=torch.long, device=device))
                    src_offs_tok = torch.tensor(src_offs_list, dtype=torch.long, device=device)
                    tgt_offs_tok = torch.tensor(tgt_offs_list, dtype=torch.long, device=device)
                    src_sigs_tok = torch.cat(src_sigs_list, dim=0) if src_sigs_list else torch.empty(0, 64, dtype=torch.long, device=device)
                    tgt_sigs_tok = torch.cat(tgt_sigs_list, dim=0) if tgt_sigs_list else torch.empty(0, 64, dtype=torch.long, device=device)
                    logits = model(src, tgt_in, src_vals_tok, src_offs_tok, tgt_vals_tok, tgt_offs_tok, src_sigs_tok, tgt_sigs_tok)
                    _seen += src.size(0)
                else:
                    logits = model(src, tgt_in)
                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))
                opt.zero_grad(set_to_none=True)
                loss.backward()
                opt.step()
                # naive greedy decode for quick metric (toy)
                pred = logits.argmax(dim=-1)
                refs.extend([[str(int(x)) for x in row.tolist()] for row in tgt.cpu()])
                hyps.extend([[str(int(x)) for x in row.tolist()] for row in pred.detach().cpu()])
        bleu = corpus_bleu(refs, hyps)
        rg = rouge_l(refs, hyps)
        msg = f"[Seq2Seq-Text][{args.attn}] epoch {ep:02d} loss {loss.item():.4f} | BLEU {bleu:.3f} | ROUGE-L {rg:.3f} | time {prof['time_s']:.2f}s"
        if prof.get("cpu_pct") is not None:
            msg += f" | CPU {prof['cpu_pct']:.1f}%"
        if torch.cuda.is_available():
            if prof.get("gpu_peak_mem_mib") is not None:
                msg += f" | VRAM {prof['gpu_peak_mem_mib']:.1f} MiB"
            if prof.get("gpu_util_pct") is not None:
                msg += f" | GPU {prof['gpu_util_pct']:.1f}%"
            if prof.get("gpu_mem_util_pct") is not None:
                msg += f" | GPU-MEM {prof['gpu_mem_util_pct']:.1f}%"
            if prof.get("gpu_active_s") is not None:
                msg += f" | GPU-ACT {prof['gpu_active_s']:.2f}s"
        print(msg)


if __name__ == "__main__":
    main()
